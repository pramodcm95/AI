{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name : Pramod Mahajan Chikkaballekere Manjunatha\n",
    "\n",
    "Collaborators: Praneeth Balakrishna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    # The code to download the mnist data original came from\n",
    "    # https://cntk.ai/pythondocs/CNTK_103A_MNIST_DataLoader.html\n",
    "    \n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import struct\n",
    "\n",
    "    try: \n",
    "        from urllib.request import urlretrieve \n",
    "    except ImportError: \n",
    "        from urllib import urlretrieve\n",
    "\n",
    "    def load_data(src, num_samples):\n",
    "        print(\"Downloading \" + src)\n",
    "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
    "        print(\"Done.\")\n",
    "        try:\n",
    "            with gzip.open(gzfname) as gz:\n",
    "                n = struct.unpack(\"I\", gz.read(4))\n",
    "                # Read magic number.\n",
    "                if n[0] != 0x3080000:\n",
    "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
    "                # Read number of entries.\n",
    "                n = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                if n != num_samples:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected {0} entries.\".format(num_samples)\n",
    "                    )\n",
    "                crow = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                ccol = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                if crow != 28 or ccol != 28:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected 28 rows/cols per image.\"\n",
    "                    )\n",
    "                # Read data.\n",
    "                res = np.frombuffer(\n",
    "                    gz.read(num_samples * crow * ccol), dtype=np.uint8\n",
    "                )\n",
    "        finally:\n",
    "            os.remove(gzfname)\n",
    "        return res.reshape((num_samples, crow, ccol)) / 256\n",
    "\n",
    "\n",
    "    def load_labels(src, num_samples):\n",
    "        print(\"Downloading \" + src)\n",
    "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
    "        print(\"Done.\")\n",
    "        try:\n",
    "            with gzip.open(gzfname) as gz:\n",
    "                n = struct.unpack(\"I\", gz.read(4))\n",
    "                # Read magic number.\n",
    "                if n[0] != 0x1080000:\n",
    "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
    "                # Read number of entries.\n",
    "                n = struct.unpack(\">I\", gz.read(4))\n",
    "                if n[0] != num_samples:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected {0} rows.\".format(num_samples)\n",
    "                    )\n",
    "                # Read labels.\n",
    "                res = np.frombuffer(gz.read(num_samples), dtype=np.uint8)\n",
    "        finally:\n",
    "            os.remove(gzfname)\n",
    "        return res.reshape((num_samples))\n",
    "\n",
    "\n",
    "    def try_download(data_source, label_source, num_samples):\n",
    "        data = load_data(data_source, num_samples)\n",
    "        labels = load_labels(label_source, num_samples)\n",
    "        return data, labels\n",
    "    \n",
    "    \n",
    "    # Not sure why, but yann lecun's website does no longer support \n",
    "    # simple downloader. (e.g. urlretrieve and wget fail, while curl work)\n",
    "    # Since not everyone has linux, use a mirror from uni server.\n",
    "    #     server = 'http://yann.lecun.com/exdb/mnist'\n",
    "    server = 'https://raw.githubusercontent.com/fgnt/mnist/master'\n",
    "    \n",
    "    # URLs for the train image and label data\n",
    "    url_train_image = f'{server}/train-images-idx3-ubyte.gz'\n",
    "    url_train_labels = f'{server}/train-labels-idx1-ubyte.gz'\n",
    "    num_train_samples = 60000\n",
    "\n",
    "    print(\"Downloading train data\")\n",
    "    train_features, train_labels = try_download(url_train_image, url_train_labels, num_train_samples)\n",
    "\n",
    "    # URLs for the test image and label data\n",
    "    url_test_image = f'{server}/t10k-images-idx3-ubyte.gz'\n",
    "    url_test_labels = f'{server}/t10k-labels-idx1-ubyte.gz'\n",
    "    num_test_samples = 10000\n",
    "\n",
    "    print(\"Downloading test data\")\n",
    "    test_features, test_labels = try_download(url_test_image, url_test_labels, num_test_samples)\n",
    "    \n",
    "    return train_features, train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train data\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading test data\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-labels-idx1-ubyte.gz\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels, test_features, test_labels = get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = train_features.reshape(-1, 28*28)\n",
    "test_images = test_features.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, value, operation):\n",
    "        self.value = np.array(value)\n",
    "        self.operation = operation\n",
    "    \n",
    "class Parameter(Variable):\n",
    "    \"\"\"\n",
    "    This class should be used for Variables that are learnable.\n",
    "    You can later use this class to distinguish learnable variables\n",
    "    from other variables (`isinstance(variable, Parameter)`).\n",
    "    \"\"\"\n",
    "    def __init__(self, value):\n",
    "        super().__init__(value, operation=None)\n",
    "        self.gradient = np.zeros_like(self.value)\n",
    "        \n",
    "class Input(Variable):\n",
    "    \"\"\"\n",
    "    This class should be used as wrapper for inputs that are not learnable.\n",
    "    \"\"\"\n",
    "    def __init__(self, value):\n",
    "        super().__init__(value, operation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.parameters = []\n",
    "    \n",
    "    def apply(self, X):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def grad(self,D):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_param(self, values):\n",
    "        param = Parameter(values)\n",
    "        self.parameters.append(param)\n",
    "        return param\n",
    "    \n",
    "    def update_parameters(self, optimizer):\n",
    "        for param in self.parameters:\n",
    "            optimizer.update(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Layer):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for layer in layers:\n",
    "            self.parameters.extend(layer.parameters) \n",
    "        \n",
    "    def apply(self, X):\n",
    "        backprops = []\n",
    "        oper = X\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            oper, backprop = layer.apply(oper)\n",
    "            backprops.append(backprop)\n",
    "\n",
    "        def grad(D):\n",
    "            for backprop in reversed(backprops):\n",
    "                D = backprop(D)\n",
    "            return D\n",
    "        \n",
    "        return oper , grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineLayer(Layer):\n",
    "    def __init__(self, in_units, out_units):\n",
    "        super().__init__()\n",
    "        least = 0.01\n",
    "        w_values = np.random.uniform(size=[in_units, out_units],\n",
    "                    low=-least,\n",
    "                    high=least\n",
    "                    )\n",
    "        self.W = self.add_param(w_values)\n",
    "        self.b = self.add_param(np.zeros(shape=out_units))\n",
    "        \n",
    "    def apply(self, X):\n",
    "        \n",
    "        def grad(D):\n",
    "            self.W.gradient = np.einsum('ji,jk->ik',X,D)\n",
    "            self.b.gradient = np.sum(D,axis=0)\n",
    "            bck_val = np.einsum('ij,kj->ik',D,self.W.value)\n",
    "            return bck_val\n",
    "        \n",
    "        frd_val = np.einsum('ij,jk->ik',X,self.W.value) + self.b.value # adding bias\n",
    "       \n",
    "       \n",
    "        return frd_val, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Relu activation layer\n",
    "class ReLU(Layer):\n",
    "    def apply(self, X):\n",
    "        z = np.maximum(X, 0)  # REPLACE z = ???\n",
    "        \n",
    "        def grad(D):\n",
    "            gx = D.copy()\n",
    "            \n",
    "            # Modify `gx` such that the gradient is corrected for x < 0.\n",
    "            gx[X < 0] = 0  # REPLACE ???\n",
    "            return gx\n",
    "\n",
    "        return z , grad\n",
    "    \n",
    "# define sigmoid activation layer    \n",
    "class Sigmoid(Layer):\n",
    "    def apply(self, X):\n",
    "        oper = 1/(1+np.exp(-X))\n",
    "        \n",
    "        def gradient(D):\n",
    "            return D \n",
    "        \n",
    "        return oper, gradient\n",
    "    \n",
    "# define optimizer as SGD\n",
    "class SGD():\n",
    "    def __init__(self, lr=0.001):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, param):\n",
    "        param.value = param.value - self.lr * param.gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cross entropy loss\n",
    "def cross_entropy(predictions, targets, epsilon=1e-11):\n",
    "    ce = -(np.sum(targets*np.log(predictions)))   # sum(p * logpi)\n",
    "    error = predictions - targets  \n",
    "    return ce , error  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot to specify mulitclass classification    \n",
    "def one_hot_encoder(train_labels):\n",
    "    r = train_labels.shape[0]\n",
    "    one_hot = np.zeros((r, 10))\n",
    "    for i in range(r):\n",
    "        one_hot[i][train_labels[i]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST():\n",
    "    def __init__(self, model, loss_function, optimizer):\n",
    "        self.model = model\n",
    "        self.loss = loss_function\n",
    "        self.optimizer = optimizer\n",
    "      \n",
    "    # batch_size per update(specified later in training)\n",
    "    def train_as_batch(self,X_train,Y_train):\n",
    "        Y_train_, gradient = self.model.apply(X_train)\n",
    "        Loss , D = self.loss(Y_train_, Y_train)          \n",
    "        gradient(D)\n",
    "        self.model.update_parameters(self.optimizer)\n",
    "        return Loss\n",
    "  \n",
    "    def train(self, X_train, Y_train, batch_size , epochs):\n",
    "        loss = []    # empty array to append loss values, which can be plotted later for better visualization\n",
    "        for epoch in range(epochs):\n",
    "            p = np.random.permutation(len(X_train))\n",
    "            L_coll = 0\n",
    "            for val in range(0, len(X_train), batch_size):\n",
    "                X_batch = X_train[p[val:val + batch_size]]\n",
    "                Y_batch = Y_train[p[val:val + batch_size]]\n",
    "                L_coll += self.train_as_batch(X_batch, Y_batch)\n",
    "            loss.append(L_coll)\n",
    "        return loss\n",
    "    \n",
    "    # predicts accuracy of my model's prediction compared to actual output\n",
    "    def accuracy(y_pred, y_test):\n",
    "        correct_predictions = np.count_nonzero(y_pred==y_test)\n",
    "        acc = correct_predictions / (y_test.shape[0]/100)\n",
    "        print(\"The Model has achieved the accuracy of \" + str(acc) + \"% on test dataset.\")\n",
    "        return acc\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        ypred, nothing = self.model.apply(X_test)\n",
    "        return ypred.argmax(axis=-1)\n",
    "    \n",
    "    def dump_parameters(self,file):\n",
    "        with open(file, 'w') as fp:\n",
    "            json.dump([p.value.tolist() for p in self.model.parameters],fp)\n",
    "            print('Wrote the parameters')\n",
    "            \n",
    "    def load_parameters(self, file):\n",
    "        with open(file) as fp:\n",
    "            parameters_values = json.load(fp)\n",
    "        for p, p_value in zip(self.model.parameters, parameters_values):\n",
    "            p.value[...] = p_value\n",
    "        print('Loaded the parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model with 1 input layer, 1 hidden layer with 500 neurons, output layer of 10 neurons, one for each of 10 digits\n",
    "neurons_in_layer = 500\n",
    "lr = 0.008   # Learning rate\n",
    "epochs = 35\n",
    "batch_size = 32\n",
    "\n",
    "X_train = training_images\n",
    "\n",
    "y_train = train_labels\n",
    "Y_train = one_hot_encoder(train_labels) \n",
    "\n",
    "X_test = test_images\n",
    "Y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new empty model here to load weights\n",
    "hidden_neurons = 500\n",
    "lr = 0.008\n",
    "\n",
    "val_model = MNIST(\n",
    "    Sequential(\n",
    "        AffineLayer(784, hidden_neurons), ReLU(), \n",
    "        AffineLayer(hidden_neurons, 10),Sigmoid()), \n",
    "    loss_function = cross_entropy, \n",
    "    optimizer = SGD(lr=lr)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the parameters\n"
     ]
    }
   ],
   "source": [
    "# load weights from a saved json file in a new model\n",
    "val_model.load_parameters(\"wt_final.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model has achieved the accuracy of 98.57% on test dataset.\n"
     ]
    }
   ],
   "source": [
    "# use the new empty model with weights loaded from json to make prediction\n",
    "val_model_accuracy = MNIST.accuracy(val_model.predict(X_test), Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
